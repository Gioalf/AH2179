{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTy+n0+EcZ4CJoBm4gZMFY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gioalf/AH2179/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Part 1 - Bus arrival delays regression modeling**"
      ],
      "metadata": {
        "id": "kHtYXY2B0Y0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_2_regression_model/Exercise2BusData.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# df = pd.read_csv('Exercise2BusData.csv')\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "w4Y35c9h0ubU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to check **different approaches**, one that exploits the large database we have with **all the 2179 entries**, and others that only use a **limited number** of random entries to check how large of a difference we get.\n"
      ],
      "metadata": {
        "id": "2ANuYgKs2nPB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-EmRGuNmDsz"
      },
      "outputs": [],
      "source": [
        "#Remove columns that are no longer needed\n",
        "df = df.drop(['Arrival_time','Stop_id','Bus_id','Line_id'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll write a short loop that runs the linear regression with the different number of rows we select."
      ],
      "metadata": {
        "id": "n8lf9phE5aJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "metrics = pd.DataFrame(columns=['method','Number of rows', 'MAE', 'MSE', 'R2'])\n",
        "\n",
        "for i, num in enumerate([50, 200, 500, 2179]): #select the different number entries to evaluate\n",
        "  df_sample = df.sample(n=num, random_state=42) #creating the sample df with the random entries\n",
        "\n",
        "  x = df_sample.drop(['Arrival_delay'], axis=1) # selecting the independent variables\n",
        "  y = df_sample['Arrival_delay'] #selecting the dependent variable\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) #train and test pools\n",
        "\n",
        "  # Create a Linear Regression model\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train, y_train)# Fit the model to the training data\n",
        "  y_pred = model.predict(X_test)# Predict the test data with the fitted model\n",
        "\n",
        "  #evaluation of the metrics\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "  #Storing the values\n",
        "  metrics.loc[i]=['Lin reg',num,mae,mse,r2]"
      ],
      "metadata": {
        "id": "jBnffLpL5Y9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's now try and use XGBoost**"
      ],
      "metadata": {
        "id": "QmBw6O8eyEpT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a59e3dd"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Defining the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 500, 1000],\n",
        "    'learning_rate': [0.01, 0.02, 0.5],\n",
        "    'max_depth': [2, 3, 5]\n",
        "}\n",
        "\n",
        "# creating an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "# Use the full dataframe\n",
        "x = df.drop(['Arrival_delay'], axis=1)\n",
        "y = df['Arrival_delay']\n",
        "\n",
        "# train and testing\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train_full, y_train_full)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# prediction\n",
        "y_pred_xgb = best_xgb_model.predict(X_test_full)\n",
        "\n",
        "# Evaluate the best model\n",
        "mae_xgb = mean_absolute_error(y_test_full, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test_full, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test_full, y_pred_xgb)\n",
        "\n",
        "metrics.loc[len(metrics)]=['XGBoost', len(x), mae_xgb, mse_xgb, r2_xgb]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparison of the different methods"
      ],
      "metadata": {
        "id": "i_CH6Qn81V2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "id": "AAKXAuC2Bbcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear regression and XGboost at the same sample size perform quite similarly in terms of error metrics. Let's see a visualization of how the two perform in a *predicted v. actual* plot."
      ],
      "metadata": {
        "id": "jJpYpi0m1oUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot for Linear regression and XGBoost\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='Linear')\n",
        "plt.scatter(y_test_full, y_pred_xgb, color='green', alpha=0.5, label='XGBoost')\n",
        "\n",
        "# Add a diagonal line representing perfect predictions\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Ideal reference')\n",
        "\n",
        "plt.xlabel('Actual Arrival Delay')\n",
        "plt.ylabel('Predicted Arrival Delay')\n",
        "plt.title('Actual vs. Predicted Arrival Delay with full sample size(Linear regression and XGBoost)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6pF0F_3FqPD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The following metrics show the importance of the the different parameters in the estimation of Arrival delays for both methods.**"
      ],
      "metadata": {
        "id": "EJYEIlEv2IgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance = pd.Series(model.coef_, index=x.columns)\n",
        "print(importance.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "ijKECYss1Kbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb.plot_importance(best_xgb_model, importance_type='weight')"
      ],
      "metadata": {
        "id": "wJ-aZdSVziz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can see that even with tuned hyperparameters the metrics show that **XGboost** performs in a **comparable** way to a simple **regression**. Furthermore the two methods show slightly different metrics, in the table above we see the **coefficients** of each feature in the linear equation. In the bar chart below we see the amount of **trees** in which each feature is used to split the data"
      ],
      "metadata": {
        "id": "64FQJddjF0Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Part 2 - Bike sharing demand prediction**"
      ],
      "metadata": {
        "id": "AcE_zgw8I_mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/zhenliangma/Applied-AI-in-Transportation/master/Exercise_2_regression_model/Exercise2BikeSharing.csv'\n",
        "df_2 = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "JWD82zYbD5jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2.head()"
      ],
      "metadata": {
        "id": "zy99Dqg5JpbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define features and target\n",
        "X = df_2.drop(['instant', 'dteday', 'casual', 'registered', 'cnt'], axis=1)\n",
        "y = df_2['cnt']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Q50XJeU8JxkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "FHXmYVw7LELX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's try some gridsearch on support vector regression**"
      ],
      "metadata": {
        "id": "QsgsIfKtO-ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning (I chose the ones already picked in the example)\n",
        "param_grid = {\n",
        "    'kernel': ['linear',  'rbf'],\n",
        "    'C': [ 1, 10],\n",
        "    'epsilon': [ 1, 10]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(SVR(), param_grid, cv=5, verbose=2)\n",
        "\n",
        "# Fit the grid search to the scaled training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "v7oHLzfULGRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an SVR model with the best parameters from the grid search\n",
        "best_svr = SVR(kernel=best_params['kernel'], C=best_params['C'], epsilon=best_params['epsilon'])\n",
        "best_svr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UQJV-HzQO9tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_svr.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "d10pTrRFPj7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we try with XGBoost then we can compare the two**"
      ],
      "metadata": {
        "id": "fyP9c9CmP_Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an XGBoost Regressor model\n",
        "xgb_reg = xgb.XGBRegressor(random_state=42, learning_rate=0.1, n_estimators=1000, max_depth=6)\n",
        "# Fit the model to the scaled training data\n",
        "xgb_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_xgb = xgb_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)"
      ],
      "metadata": {
        "id": "g0QRP25_QOzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary with the metrics for each model\n",
        "metrics_data = {\n",
        "    'Model': ['SVR', 'XGBoost'],\n",
        "    'MAE': [mae, mae_xgb],\n",
        "    'MSE': [mse, mse_xgb],\n",
        "    'R2': [r2, r2_xgb]\n",
        "}\n",
        "\n",
        "# Create a pandas DataFrame from the dictionary\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(metrics_df)"
      ],
      "metadata": {
        "id": "n7qbrNcSQ2Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd58208f"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot for SVR and XGBoost\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5, label='SVR')\n",
        "plt.scatter(y_test, y_pred_xgb, color='green', alpha=0.5, label='XGBoost')\n",
        "\n",
        "# Add a diagonal line representing perfect predictions\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Ideal reference')\n",
        "\n",
        "plt.xlabel('Actual Bike Rentals')\n",
        "plt.ylabel('Predicted Bike Rentals')\n",
        "plt.title('Actual vs. Predicted Bike Rentals (SVR and XGBoost)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this case we can see that the XGBoost regressor has far better performance both in terms of metrics and time taken to execute, despite not having finetuned the hyperparameters.**\n",
        "\n"
      ],
      "metadata": {
        "id": "aDASBnaHSRZ1"
      }
    }
  ]
}